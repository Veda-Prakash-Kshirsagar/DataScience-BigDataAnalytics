{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"toc_visible":true,"authorship_tag":"ABX9TyP1KC9bHv5e2ubiBOlLuT2f"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":31,"metadata":{"id":"Dmg9TsKqtP0_","executionInfo":{"status":"ok","timestamp":1712430696898,"user_tz":-330,"elapsed":492,"user":{"displayName":"Veda Kshirsagar","userId":"16091712550875204102"}}},"outputs":[],"source":["#Veda Kshirsagr\n","#TE_A_61\n","#DSBDA_7"]},{"cell_type":"code","source":["pip install nltk"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":0},"id":"xyEuiKGQ0VyS","executionInfo":{"status":"ok","timestamp":1712430626032,"user_tz":-330,"elapsed":11308,"user":{"displayName":"Veda Kshirsagar","userId":"16091712550875204102"}},"outputId":"6198f88d-5ca2-49bf-b424-3faa97150b1c"},"execution_count":30,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n","Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.3.2)\n","Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2023.12.25)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.2)\n"]}]},{"cell_type":"code","source":["import nltk"],"metadata":{"id":"_ZquNbiTtkO3","executionInfo":{"status":"ok","timestamp":1712428868849,"user_tz":-330,"elapsed":2114,"user":{"displayName":"Veda Kshirsagar","userId":"16091712550875204102"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["nltk.download('punkt')\n","nltk.download('stopwords')\n","nltk.download('wordnet')\n","nltk.download('averaged_perceptron_tagger')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":0},"id":"X6b-w6xyttaY","executionInfo":{"status":"ok","timestamp":1712428972353,"user_tz":-330,"elapsed":1345,"user":{"displayName":"Veda Kshirsagar","userId":"16091712550875204102"}},"outputId":"82030971-2eb7-4ae5-885a-4b30579e7fdd"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n","[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data] Downloading package averaged_perceptron_tagger to\n","[nltk_data]     /root/nltk_data...\n","[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":3}]},{"cell_type":"code","source":["text = \"In Python tokenization basically refers to splitting up a larger body of text into smaller lines, words or even creating words for a non-English language. The various tokenization functions in-built into the nltk module itself and can be used in programs as shown below.\""],"metadata":{"id":"R4FqL4h4uHCQ","executionInfo":{"status":"ok","timestamp":1712429162086,"user_tz":-330,"elapsed":477,"user":{"displayName":"Veda Kshirsagar","userId":"16091712550875204102"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["from nltk.tokenize import sent_tokenize\n","tokenized_text = sent_tokenize(text)"],"metadata":{"id":"agqCSw3PuVyH","executionInfo":{"status":"ok","timestamp":1712429164916,"user_tz":-330,"elapsed":1,"user":{"displayName":"Veda Kshirsagar","userId":"16091712550875204102"}}},"execution_count":8,"outputs":[]},{"cell_type":"code","source":["print(tokenized_text)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":0},"id":"_B6kuDwsum-V","executionInfo":{"status":"ok","timestamp":1712429177268,"user_tz":-330,"elapsed":15,"user":{"displayName":"Veda Kshirsagar","userId":"16091712550875204102"}},"outputId":"28373c42-f5d9-4ed1-f604-a5290da84e17"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["['In Python tokenization basically refers to splitting up a larger body of text into smaller lines, words or even creating words for a non-English language.', 'The various tokenization functions in-built into the nltk module itself and can be used in programs as shown below.']\n"]}]},{"cell_type":"code","source":["from nltk.tokenize import word_tokenize\n","tokenized_word = word_tokenize(text)"],"metadata":{"id":"SxZYTjUUu5ON","executionInfo":{"status":"ok","timestamp":1712429259627,"user_tz":-330,"elapsed":662,"user":{"displayName":"Veda Kshirsagar","userId":"16091712550875204102"}}},"execution_count":10,"outputs":[]},{"cell_type":"code","source":["print(tokenized_word)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":0},"id":"bJ2qeXNGvNVW","executionInfo":{"status":"ok","timestamp":1712429276861,"user_tz":-330,"elapsed":2,"user":{"displayName":"Veda Kshirsagar","userId":"16091712550875204102"}},"outputId":"d3a318c7-09e7-49dd-f063-62763acb3428"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["['In', 'Python', 'tokenization', 'basically', 'refers', 'to', 'splitting', 'up', 'a', 'larger', 'body', 'of', 'text', 'into', 'smaller', 'lines', ',', 'words', 'or', 'even', 'creating', 'words', 'for', 'a', 'non-English', 'language', '.', 'The', 'various', 'tokenization', 'functions', 'in-built', 'into', 'the', 'nltk', 'module', 'itself', 'and', 'can', 'be', 'used', 'in', 'programs', 'as', 'shown', 'below', '.']\n"]}]},{"cell_type":"code","source":["from nltk.corpus import stopwords\n","stop_words = set(stopwords.words(\"english\"))"],"metadata":{"id":"G_Vz6qKMvRjH","executionInfo":{"status":"ok","timestamp":1712429337553,"user_tz":-330,"elapsed":2,"user":{"displayName":"Veda Kshirsagar","userId":"16091712550875204102"}}},"execution_count":13,"outputs":[]},{"cell_type":"code","source":["print(stop_words)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":0},"id":"faxSWihWvZ5x","executionInfo":{"status":"ok","timestamp":1712429349919,"user_tz":-330,"elapsed":3,"user":{"displayName":"Veda Kshirsagar","userId":"16091712550875204102"}},"outputId":"ea40e648-a997-4f8e-9850-99b741f73056"},"execution_count":14,"outputs":[{"output_type":"stream","name":"stdout","text":["{'ourselves', 'some', 'who', 'my', 'other', 'than', 'same', 'mustn', \"don't\", \"won't\", 'them', 'and', 'been', 'were', 'you', 'into', 'own', 'so', 'their', 'a', 'to', \"weren't\", 've', 't', 'should', 'herself', 'wasn', 'as', 'yours', 'from', 'here', 'during', 're', 'he', 'theirs', 'can', 'at', 'an', 'hadn', 'such', 'with', 'further', 'because', 'will', 'aren', 'haven', 'isn', \"hasn't\", 'ours', 'mightn', 'has', 'being', \"hadn't\", 'these', 'do', 'we', 'they', 'those', 'between', 'all', 'yourselves', 'not', 'does', 'just', 'there', 'your', 'themselves', 'up', 'for', 'why', 'down', 'that', 'while', 'again', \"wouldn't\", 'me', 'any', 'once', 'above', 'over', 'll', 'in', 'don', 'hers', 'no', \"mightn't\", \"should've\", \"doesn't\", 'shouldn', 'how', 'ma', 'which', 'its', \"isn't\", 'are', 'very', \"she's\", 'when', \"needn't\", 'she', 'wouldn', 'her', 'is', 'under', \"you've\", 'then', 'y', 'but', \"aren't\", \"shan't\", 'him', 'himself', 'myself', \"wasn't\", 'd', 'below', 'most', 'needn', 'it', 'weren', 'am', 'off', 'nor', 'out', 's', 'yourself', 'having', \"you'd\", 'was', \"mustn't\", 'didn', 'more', 'until', 'doing', \"it's\", 'what', 'of', 'after', 'his', 'our', 'only', 'couldn', 'or', 'both', 'where', 'the', 'itself', 'each', \"couldn't\", 'did', 'on', 'shan', 'before', 'this', 'now', 'doesn', 'too', 'by', 'few', 'be', 'i', 'won', \"didn't\", 'o', 'whom', 'have', 'about', \"shouldn't\", \"haven't\", 'against', 'hasn', 'had', 'if', \"that'll\", \"you're\", 'ain', 'through', 'm', \"you'll\"}\n"]}]},{"cell_type":"code","source":["import re  #Regular Expression"],"metadata":{"id":"tPrIykuwvjbt","executionInfo":{"status":"ok","timestamp":1712429428672,"user_tz":-330,"elapsed":3,"user":{"displayName":"Veda Kshirsagar","userId":"16091712550875204102"}}},"execution_count":16,"outputs":[]},{"cell_type":"code","source":["text = \"Tokenization is breaking down a text into smaller pieces called “tokens.” These tokens can be words, phrases, or even letters or numbers. Tokenization is often used in natural language processing (NLP), a way for computers to understand and analyze human language. Tokenization helps computers understand the meaning of words and how they relate to each other in a sentence. It can be like taking a puzzle apart and putting it together again, except with words instead of puzzle pieces!\"\n","text = re.sub('[^a-zA-Z]',' ',text)\n","tokens = word_tokenize(text.lower())"],"metadata":{"id":"RADEMjTTvuux","executionInfo":{"status":"ok","timestamp":1712429730913,"user_tz":-330,"elapsed":2,"user":{"displayName":"Veda Kshirsagar","userId":"16091712550875204102"}}},"execution_count":21,"outputs":[]},{"cell_type":"code","source":["filtered_text = []\n","for w in tokens:\n","  if w not in stop_words:\n","    filtered_text.append(w)\n","\n","print(\"Tokenized sentence: \", tokens)\n","print(\"Filtered sentence: \", filtered_text)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":0},"id":"g2y4Eey7wfWy","executionInfo":{"status":"ok","timestamp":1712429852140,"user_tz":-330,"elapsed":11,"user":{"displayName":"Veda Kshirsagar","userId":"16091712550875204102"}},"outputId":"3b00d12e-f769-4ce5-a819-a2e69b811f6c"},"execution_count":22,"outputs":[{"output_type":"stream","name":"stdout","text":["Tokenized sentence:  ['tokenization', 'is', 'breaking', 'down', 'a', 'text', 'into', 'smaller', 'pieces', 'called', 'tokens', 'these', 'tokens', 'can', 'be', 'words', 'phrases', 'or', 'even', 'letters', 'or', 'numbers', 'tokenization', 'is', 'often', 'used', 'in', 'natural', 'language', 'processing', 'nlp', 'a', 'way', 'for', 'computers', 'to', 'understand', 'and', 'analyze', 'human', 'language', 'tokenization', 'helps', 'computers', 'understand', 'the', 'meaning', 'of', 'words', 'and', 'how', 'they', 'relate', 'to', 'each', 'other', 'in', 'a', 'sentence', 'it', 'can', 'be', 'like', 'taking', 'a', 'puzzle', 'apart', 'and', 'putting', 'it', 'together', 'again', 'except', 'with', 'words', 'instead', 'of', 'puzzle', 'pieces']\n","Filtered sentence:  ['tokenization', 'breaking', 'text', 'smaller', 'pieces', 'called', 'tokens', 'tokens', 'words', 'phrases', 'even', 'letters', 'numbers', 'tokenization', 'often', 'used', 'natural', 'language', 'processing', 'nlp', 'way', 'computers', 'understand', 'analyze', 'human', 'language', 'tokenization', 'helps', 'computers', 'understand', 'meaning', 'words', 'relate', 'sentence', 'like', 'taking', 'puzzle', 'apart', 'putting', 'together', 'except', 'words', 'instead', 'puzzle', 'pieces']\n"]}]},{"cell_type":"code","source":["from nltk.stem import WordNetLemmatizer\n","wordnet_lemmatizer = WordNetLemmatizer()\n","text = \"studies studying cries crying car caring\"\n","tokenization = nltk.word_tokenize(text)\n","for w in tokenization:\n","  print(w,wordnet_lemmatizer.lemmatize(w))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":0},"id":"6MCkHHYoxd9n","executionInfo":{"status":"ok","timestamp":1712430058964,"user_tz":-330,"elapsed":1815,"user":{"displayName":"Veda Kshirsagar","userId":"16091712550875204102"}},"outputId":"1f1368ea-ca55-4215-81b8-4a1746c0932a"},"execution_count":24,"outputs":[{"output_type":"stream","name":"stdout","text":["studies study\n","studying studying\n","cries cry\n","crying cry\n","car car\n","caring caring\n"]}]},{"cell_type":"code","source":["from nltk.tokenize import word_tokenize\n","data = \"The black shirt fits him perfectly\"\n","words = word_tokenize(data)\n","for word in words:\n","  print(nltk.pos_tag([word]))\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":0},"id":"yOtxz4KpyBak","executionInfo":{"status":"ok","timestamp":1712430244273,"user_tz":-330,"elapsed":14,"user":{"displayName":"Veda Kshirsagar","userId":"16091712550875204102"}},"outputId":"580ec798-c433-4e61-cd48-2ad8d9b80d19"},"execution_count":25,"outputs":[{"output_type":"stream","name":"stdout","text":["[('The', 'DT')]\n","[('black', 'JJ')]\n","[('shirt', 'NN')]\n","[('fits', 'NNS')]\n","[('him', 'PRP')]\n","[('perfectly', 'RB')]\n"]}]},{"cell_type":"code","source":["para = \"Tokenization is breaking down a text into smaller pieces called “tokens.” These tokens can be words, phrases, or even letters or numbers. Tokenization is often used in natural language processing (NLP), a way for computers to understand and analyze human language. Tokenization helps computers understand the meaning of words and how they relate to each other in a sentence. It can be like taking a puzzle apart and putting it together again, except with words.\"\n","wn = WordNetLemmatizer()\n","sentences = nltk.sent_tokenize(para)\n","sentences"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":0},"id":"a1W_hkVgy9y_","executionInfo":{"status":"ok","timestamp":1712430554221,"user_tz":-330,"elapsed":778,"user":{"displayName":"Veda Kshirsagar","userId":"16091712550875204102"}},"outputId":"b89c2961-62c5-4ffa-8671-f705b441b7c2"},"execution_count":28,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['Tokenization is breaking down a text into smaller pieces called “tokens.” These tokens can be words, phrases, or even letters or numbers.',\n"," 'Tokenization is often used in natural language processing (NLP), a way for computers to understand and analyze human language.',\n"," 'Tokenization helps computers understand the meaning of words and how they relate to each other in a sentence.',\n"," 'It can be like taking a puzzle apart and putting it together again, except with words.']"]},"metadata":{},"execution_count":28}]},{"cell_type":"code","source":["from sklearn.feature_extraction.text import TfidfVectorizer\n","tf = TfidfVectorizer()\n","x = tf.fit_transform(sentences).toarray()\n","print(x)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":0},"id":"cOSG9Vcizjaf","executionInfo":{"status":"ok","timestamp":1712430560793,"user_tz":-330,"elapsed":480,"user":{"displayName":"Veda Kshirsagar","userId":"16091712550875204102"}},"outputId":"9b921951-3042-42a2-c39e-c2947439e47d"},"execution_count":29,"outputs":[{"output_type":"stream","name":"stdout","text":["[[0.         0.         0.         0.         0.16555232 0.20998212\n","  0.20998212 0.16555232 0.         0.20998212 0.         0.20998212\n","  0.         0.         0.         0.         0.         0.\n","  0.20998212 0.16555232 0.         0.         0.20998212 0.\n","  0.         0.         0.         0.20998212 0.         0.\n","  0.41996424 0.         0.20998212 0.20998212 0.         0.\n","  0.         0.         0.         0.20998212 0.         0.20998212\n","  0.         0.20998212 0.         0.         0.         0.13402887\n","  0.41996424 0.         0.         0.         0.         0.13402887]\n"," [0.         0.24308832 0.15516013 0.         0.         0.\n","  0.         0.         0.19165362 0.         0.         0.\n","  0.         0.24308832 0.         0.         0.24308832 0.19165362\n","  0.         0.19165362 0.         0.48617664 0.         0.\n","  0.         0.24308832 0.24308832 0.         0.         0.24308832\n","  0.         0.         0.         0.         0.24308832 0.\n","  0.         0.         0.         0.         0.         0.\n","  0.         0.         0.         0.19165362 0.         0.15516013\n","  0.         0.19165362 0.24308832 0.24308832 0.         0.        ]\n"," [0.         0.         0.17239293 0.         0.         0.\n","  0.         0.         0.21293957 0.         0.27008684 0.\n","  0.         0.         0.27008684 0.27008684 0.         0.21293957\n","  0.         0.         0.         0.         0.         0.\n","  0.27008684 0.         0.         0.         0.27008684 0.\n","  0.         0.27008684 0.         0.         0.         0.\n","  0.         0.27008684 0.27008684 0.         0.         0.\n","  0.27008684 0.         0.27008684 0.21293957 0.         0.17239293\n","  0.         0.21293957 0.         0.         0.         0.17239293]\n"," [0.2577011  0.         0.16448727 0.2577011  0.2031745  0.\n","  0.         0.2031745  0.         0.         0.         0.\n","  0.2577011  0.         0.         0.         0.         0.\n","  0.         0.         0.51540219 0.         0.         0.2577011\n","  0.         0.         0.         0.         0.         0.\n","  0.         0.         0.         0.         0.         0.2577011\n","  0.2577011  0.         0.         0.         0.2577011  0.\n","  0.         0.         0.         0.         0.2577011  0.\n","  0.         0.         0.         0.         0.2577011  0.16448727]]\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"4ZxhuUTe0DU2"},"execution_count":null,"outputs":[]}]}